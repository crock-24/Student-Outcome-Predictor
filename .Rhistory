predict(nnetFit, trainPred)
confusionMatrix(data = predict(nnetFit, trainPred), reference = trainResp)
library(mda)
ctrl <- trainControl(method = "LGOCV", summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = TRUE)
set.seed(100)
mdaFit <- train(x = trainPred, y = trainResp,method = "mda",metric = "ROC", tuneGrid = expand.grid(.subclasses = 1:4),trControl = ctrl)
mdaFit
plot(mdaFit)
confusionMatrix(data = mdaFit$pred$pred, reference = mdaFit$pred$obs)
plot(lrFull)
library(AppliedPredictiveModeling)
data(hepatic)
low.variability.columns <- nearZeroVar(bio)
library(AppliedPredictiveModeling)
library(caret)
data(hepatic)
low.variability.columns <- nearZeroVar(bio)
bio <- bio[,-low.variability.columns]
correlations <- cor(bio)
library(corrplot)
corrplot(correlations, order = "hclust")
highly.correlated.columns <- findCorrelation(correlations, cutoff = .85)
bio <- bio[,-highly.correlated.columns]
set.seed(100)
trainRows <- createDataPartition(injury, p = .80, list = FALSE)
trainResp <- injury[trainRows]
trainPred <- bio[trainRows,]
testResp <- injury[-trainRows]
testPred <- bio[-trainRows,]
#Week 7 Assignment
#Work with the same training and testing sets as well as pre-processing steps as you did in your
#previous work (12.1) on these data. Using the same classification statistic as before, build models
#described in this chapter for the biological predictors.
library(AppliedPredictiveModeling)
library(caret)
data(hepatic)
low.variability.columns <- nearZeroVar(bio)
bio <- bio[,-low.variability.columns]
correlations <- cor(bio)
library(corrplot)
corrplot(correlations, order = "hclust")
highly.correlated.columns <- findCorrelation(correlations, cutoff = .85)
bio <- bio[,-highly.correlated.columns]
set.seed(100)
trainRows <- createDataPartition(injury, p = .80, list = FALSE)
trainResp <- injury[trainRows]
trainPred <- bio[trainRows,]
testResp <- injury[-trainRows]
testPred <- bio[-trainRows,]
set.seed(100)
library(kernlab)
ctrl <- trainControl(summaryFunction = twoClassSummary,classProbs = TRUE)
sigmaRangeReduced <- sigest(as.matrix(simulatedTrain[,1:4]))
set.seed(100)
library(kernlab)
ctrl <- trainControl(summaryFunction = twoClassSummary,classProbs = TRUE)
sigmaRangeReduced <- sigest(as.matrix(trainPred))
svmRGridReduced <- expand.grid(.sigma = sigmaRangeReduced[1:3], .C = 2^(seq(-4, 6)))
set.seed(100)
svmRModel <- train(x = trainPred, y = trainResp, method = "svmRadial",metric = "ROC",preProc = c("center", "scale"),tuneGrid = svmRGridReduced,fit = FALSE,trControl = ctrl)
set.seed(100)
library(kernlab)
ctrl <- trainControl(method = 'LGOCV', number = 10, classProbs = TRUE)
sigmaRangeReduced <- sigest(as.matrix(trainPred))
svmRGridReduced <- expand.grid(.sigma = sigmaRangeReduced[1:3], .C = 2^(seq(-4, 6)))
set.seed(100)
svmRModel <- train(x = trainPred, y = trainResp, method = "svmRadial",metric = "ROC",preProc = c("center", "scale"),tuneGrid = svmRGridReduced,fit = FALSE,trControl = ctrl)
svmRModel
plot(svmRModel)
set.seed(100)
knnFit <- train(x = trainPred, y = trainResp, method = "knn",metric = "ROC", preProc = c("center", "scale"), tuneGrid = data.frame(.k = 1:30),trControl = ctrl)
knnFit
set.seed(100)
knnFit <- train(x = trainPred, y = trainResp, method = "knn",metric = "Kappa", preProc = c("center", "scale"), tuneGrid = data.frame(.k = 1:30),trControl = ctrl)
knnFit
plot(knnFit)
#Week 7 Assignment
#Work with the same training and testing sets as well as pre-processing steps as you did in your
#previous work (12.1) on these data. Using the same classification statistic as before, build models
#described in this chapter for the biological predictors.
library(AppliedPredictiveModeling)
library(caret)
data(hepatic)
low.variability.columns <- nearZeroVar(bio)
bio <- bio[,-low.variability.columns]
correlations <- cor(bio)
library(corrplot)
corrplot(correlations, order = "hclust")
highly.correlated.columns <- findCorrelation(correlations, cutoff = .85)
bio <- bio[,-highly.correlated.columns]
set.seed(100)
trainRows <- createDataPartition(injury, p = .80, list = FALSE)
trainResp <- injury[trainRows]
trainPred <- bio[trainRows,]
testResp <- injury[-trainRows]
testPred <- bio[-trainRows,]
#13.1a) Which model has the best predictive ability for the biological predictors and what is the optimal
# performance? (For each model, please put the table of the summary statistics for each tuning parameter,
# the figure for the tuning parameter, and the confusion matrix, then use a table to summarize the
# classification statistic and the best tuning parameter values for all models).
### Radial Support Vector Machine
set.seed(100)
library(kernlab)
ctrl <- trainControl(method = 'LGOCV', number = 15, classProbs = TRUE)
sigmaRangeReduced <- sigest(as.matrix(trainPred))
svmRGridReduced <- expand.grid(.sigma = sigmaRangeReduced[1:3], .C = 2^(seq(-4, 6)))
set.seed(100)
svmRModel <- train(x = trainPred, y = trainResp, method = "svmRadial",metric = "Kappa",preProc = c("center", "scale"),tuneGrid = svmRGridReduced,fit = FALSE,trControl = ctrl)
svmRModel
plot(svmRModel)
### Radial Support Vector Machine
set.seed(100)
library(kernlab)
ctrl <- trainControl(method = 'LGOCV', number = 15, classProbs = TRUE)
sigmaRangeReduced <- sigest(as.matrix(trainPred))
svmRGridReduced <- expand.grid(.sigma = sigmaRangeReduced[1:3], .C = 2^(seq(-3, 6)))
set.seed(100)
svmRModel <- train(x = trainPred, y = trainResp, method = "svmRadial",metric = "Kappa",preProc = c("center", "scale"),tuneGrid = svmRGridReduced,fit = FALSE,trControl = ctrl)
svmRModel
plot(svmRModel)
confusionMatrix(data = svmRModel$pred$pred,reference = svmRModel$pred$obs)
svmRModel$pred$pred
confusionMatrix(data = predict(svmRModel, trainPred),reference = trainResp)
confusionMatrix(data = predict(svmRModel, testPred),reference = testResp)
set.seed(100)
knnFit <- train(x = trainPred, y = trainResp, method = "knn",metric = "Kappa", preProc = c("center", "scale"), tuneGrid = data.frame(.k = 1:30),trControl = ctrl)
knnFit
plot(knnFit)
confusionMatrix(data = predict(knnFit, testPred),reference = testResp)
library(klaR)
set.seed(100)
nbFit <- train( x = simulatedTrain[,1:4], y = simulatedTrain$class, method = "nb", metric = "Kappa", preProc = c("center", "scale"),tuneGrid = data.frame(.fL = 2,.usekernel = TRUE,.adjust = TRUE),trControl = ctrl)
library(klaR)
set.seed(100)
nbFit <- train( x = trainPred, y = trainResp, method = "nb", metric = "Kappa", preProc = c("center", "scale"),tuneGrid = data.frame(.fL = 2,.usekernel = TRUE,.adjust = TRUE),trControl = ctrl)
nbFit
plot(nbFit)
nbFit
confusionMatrix(data = predict(nbFit, testPred),reference = testResp)
varImp(svmRModel)
plot(varImp(svmRModel))
#Assignment Week 6
#Cody Rorick
library(caret)
library(AppliedPredictiveModeling)
data(hepatic)
#Problem 12.1c Pre-process the data, split the data into a training and a testing set, and build models described
#in this chapter for the biological predictors. Using each model to predict on the testing set, which
#model has the best predictive ability for the biological predictors and what is the optimal
#performance?
low.variability.columns <- nearZeroVar(bio)
bio <- bio[,-low.variability.columns]
correlations <- cor(bio)
library(corrplot)
corrplot(correlations, order = "hclust")
highly.correlated.columns <- findCorrelation(correlations, cutoff = .85)
bio <- bio[,-highly.correlated.columns]
set.seed(100)
trainRows <- createDataPartition(injury, p = .80, list = FALSE)
trainResp <- injury[trainRows]
trainPred <- bio[trainRows,]
testResp <- injury[-trainRows]
testPred <- bio[-trainRows,]
### Linear Discriminant Analysis
ctrl <- trainControl(method = "LGOCV", number = 10, classProbs = TRUE, savePredictions = TRUE)
LDAFull <- train(trainPred, y = trainResp, method = "lda", preProc = c("center","scale"), metric = "Kappa", trControl = ctrl)
LDAFull
confusionMatrix(data = predict(LDAFull, testPred), reference = testResp) #average over 10*.25*1000 observations
set.seed(100)
ctrl <- trainControl(method = "LGOCV", number = 10, classProbs = TRUE, savePredictions = TRUE)
plsFit <- train(x = trainPred, y = trainResp, method = "pls",tuneGrid = expand.grid(.ncomp = 1:10), preProc = c("center","scale"), metric = "Kappa", trControl = ctrl)
plsFit
plot(plsFit)
confusionMatrix(data = predict(plsFit, testPred), reference = testResp)
library(glmnet)
ctrl <- trainControl(method = "LGOCV", number = 10, classProbs = TRUE,savePredictions = TRUE)
glmnGrid <- expand.grid(.alpha = c(0, .1, .2, .4, .6),.lambda = seq(.01, .2, length = 5))
set.seed(100)
glmnTuned <- train(trainPred, y = trainResp, method = "glmnet", tuneGrid = glmnGrid, preProc = c("center", "scale"), metric = "Kappa", trControl = ctrl)
glmnTuned
plot(glmnTuned)
confusionMatrix(data = predict(glmnTuned, testPred), reference = testResp)
varImp(glmnTuned)
#Problem 12.3a Explore the data by visualizing the relationship between the predictors and the outcome. Are
#there important features of the predictor data themselves, such as between-predictor correlations
#or degenerate distributions?
library(modeldata)
data("mlc_churn")
low.variability.columns <- nearZeroVar(mlc_churn)
hist(unlist(mlc_churn[,6]), main = 'Vmail Messages', xlab = '# Vmail Messages')
mlc_churn <- mlc_churn[,-low.variability.columns]
correlations <- cor(mlc_churn[,-c(1,3,4,5,19)])
corrplot(correlations, order = "hclust")
highly.correlated.columns <- findCorrelation(correlations, cutoff = .85)
mlc_churn <- mlc_churn[,-highly.correlated.columns]
#Problem 12.3c Split the data into training set and test set using random splitting (80% and 20%). Fit models
#covered in this chapter to the training set and tune them via resampling. Which model has the best performance?
set.seed(100)
trainRows <- createDataPartition(mlc_churn$churn, p = .80, list = FALSE)
trainResp <- mlc_churn$churn[trainRows]
trainPred <- mlc_churn[trainRows,-c(15)]
testResp <- mlc_churn$churn[-trainRows]
testPred <- mlc_churn[-trainRows,-c(15)]
### Logistic Regression
set.seed(100)
ctrl <- trainControl(method = "LGOCV", summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = TRUE)
lrFull <- train(trainPred, y = trainResp, method = "glm",preProc = c("center", "scale"), metric = "ROC", trControl = ctrl)
lrFull
confusionMatrix(data = predict(lrFull, testPred), reference = testResp)
library(glmnet)
ctrl <- trainControl(method = "LGOCV",summaryFunction = twoClassSummary,classProbs = TRUE,savePredictions = TRUE)
glmnGrid <- expand.grid(.alpha = c(0, .1, .2, .4, .6),.lambda = seq(.01, .2, length = 5))
set.seed(100)
glmnTuned <- train(trainPred, y = trainResp, method = "glmnet",tuneGrid = glmnGrid,preProc = c("center", "scale"),metric = "ROC",trControl = ctrl)
glmnTuned
plot(glmnTuned)
confusionMatrix(data = predict(glmnTuned, testPred), reference = testResp)
nnetGrid <- expand.grid(.size = 1:3, .decay = c(0, .1, .3, .5, 1))
maxSize <- max(nnetGrid$.size)
numWts <- (15 * (14 + 1) + (15+1)*2) ## 14 is the number of predictors; 2 is the number of classes; 15 is size*decay
ctrl <- trainControl(method = 'LGOCV', summaryFunction = twoClassSummary,classProbs = TRUE)
nnetFit <- train(x = trainPred, y = trainResp,method = "nnet",metric = "ROC",preProc = c("center", "scale", "spatialSign"),tuneGrid = nnetGrid,trace = FALSE,maxit = 2000,MaxNWts = numWts,trControl = ctrl)
nnetFit
plot(nnetFit)
confusionMatrix(data = predict(nnetFit, testPred), reference = testResp)
library(mda)
ctrl <- trainControl(method = "LGOCV", summaryFunction = twoClassSummary, classProbs = TRUE, savePredictions = TRUE)
set.seed(100)
mdaFit <- train(x = trainPred, y = trainResp,method = "mda",metric = "ROC", tuneGrid = expand.grid(.subclasses = 1:4),trControl = ctrl)
mdaFit
plot(mdaFit)
confusionMatrix(data = predict(mdaFit, testPred), reference = testResp)
library(pROC)
rocCurve <- roc(response = testResp, predictor = predict(lrFull, testPred))
predict(lrFull, testPred)
rocCurve <- roc(response = testResp, predictor = predict(lrFull, testPred, type = 'response'))
rocCurve <- roc(response = testResp, predictor = predict(lrFull, testPred, type = 'prob'))
predict(lrFull, testPred, type = 'prob')
rocCurve <- roc(response = testResp, predictor = predict(lrFull, testPred, type = 'prob')[,1])
rocCurve <- roc(response = testResp, predictor = predict(lrFull, testPred, type = 'prob')[1])
predict(lrFull, testPred, type = 'prob')[,1]
rocCurve <- roc(response = testResp, predictor = predict(lrFull, testPred, type = 'prob')[,1])
auc(rocCurve)
rocCurve <- roc(response = testResp, predictor = predict(lrFull, testPred, type = 'prob')[,2])
auc(rocCurve)
rocCurve <- roc(response = testResp, predictor = predict(glmnTuned, testPred, type = 'prob')[,1])
auc(rocCurve)
rocCurve <- roc(response = testResp, predictor = predict(nnetFit, testPred, type = 'prob')[,1])
auc(rocCurve)
rocCurve <- roc(response = testResp, predictor = predict(nnetFit, testPred, type = 'prob')[,2])
auc(rocCurve)
rocCurve <- roc(response = testResp, predictor = predict(mdaFit, testPred, type = 'prob')[,1])
auc(rocCurve)
# Loading in needed packages
library(caret)
library(moments)
library(corrplot)
library(AppliedPredictiveModeling)
library(ggplot2)
library(e1071)
library(glmnet)
library(MASS)
library(pamr)
library(pROC)
library(sparseLDA)
library(dplyr)
# Reading data into dataframe, data is seperated by semicolon so using that as delimeter
setwd("/Users/crorick/Documents/MS\ Applied\ Stats\ Fall\ 2023/MA5790/group_project/R_code")
studentSuccess <- read.csv('data.csv', sep = ';', header = TRUE)
# Getting a view of what the data looks like
str(studentSuccess)
studentSuccess$Target
# check distribution of response
par(mfrow = c(1,1))
hist(as.numeric(factor(studentSuccess$Target)), col = "steelblue",
main = "Distribution of Student Success",
xlab = "(1 = Dropout, 2 = Enrolled, 3 = Graduate)")
# Histogram shows very different frequencies for the various levels, a stratified sampling approach should be taken
#### pre-processing ###
# check for missing values
nas_bycol <- colSums(is.na(studentSuccess))
sum(is.na(studentSuccess))
# no missing values
# separate response from predictors
success_ouctome <- studentSuccess$Target # saves outcome separately
success_predictors <- studentSuccess
success_predictors$Target <- NULL # removes outcome from predictor set
str(success_predictors)
str(success_ouctome)
success_ouctome <- as.factor(success_ouctome)
## check skewness
# histograms of predictors
par(mfrow = c(3,4))
#hist(Previous.qualification, col = "lightblue")
hist(studentSuccess$Admission.grade, col = "darkblue")
hist(Curricular.units.1st.sem..credited., col = "cornflowerblue")
# Loading in needed packages
library(caret)
library(moments)
library(corrplot)
library(AppliedPredictiveModeling)
library(ggplot2)
library(e1071)
library(glmnet)
library(MASS)
library(pamr)
library(pROC)
library(sparseLDA)
library(dplyr)
# Reading data into dataframe, data is seperated by semicolon so using that as delimeter
setwd("/Users/crorick/Documents/MS\ Applied\ Stats\ Fall\ 2023/MA5790/group_project/R_code")
studentSuccess <- read.csv('data.csv', sep = ';', header = TRUE)
# Getting a view of what the data looks like
str(studentSuccess)
studentSuccess$Target
# check distribution of response
par(mfrow = c(1,1))
hist(as.numeric(factor(studentSuccess$Target)), col = "steelblue",
main = "Distribution of Student Success",
xlab = "(1 = Dropout, 2 = Enrolled, 3 = Graduate)")
# Histogram shows very different frequencies for the various levels, a stratified sampling approach should be taken
#### pre-processing ###
# check for missing values
nas_bycol <- colSums(is.na(studentSuccess))
sum(is.na(studentSuccess))
# no missing values
# separate response from predictors
success_ouctome <- studentSuccess$Target # saves outcome separately
success_predictors <- studentSuccess
success_predictors$Target <- NULL # removes outcome from predictor set
str(success_predictors)
str(success_ouctome)
success_ouctome <- as.factor(success_ouctome)
## check skewness
# histograms of predictors
par(mfrow = c(3,4))
#hist(Previous.qualification, col = "lightblue")
hist(studentSuccess$Admission.grade, col = "darkblue")
hist(studentSuccess$Curricular.units.1st.sem..credited., col = "cornflowerblue")
hist(studentSuccess$Curricular.units.1st.sem..evaluations., col = "darkorange")
hist(studentSuccess$Curricular.units.1st.sem..grade., col = "darkred")
hist(studentSuccess$Curricular.units.2nd.sem..credited., col = "yellow")
# hist(Curricular.units.2nd.sem..evaluations.) # not skewed trying to lower margins
hist(studentSuccess$Curricular.units.2nd.sem..grade., col = "darkslateblue")
#hist(Unemployment.rate)
#hist(GDP)
#hist(Previous.qualification..grade.)
hist(studentSuccess$Age.at.enrollment, col = "darkmagenta")
hist(studentSuccess$Curricular.units.1st.sem..enrolled., col = "purple")
par(mfrow = c(2,2))
hist(studentSuccess$Curricular.units.1st.sem..approved., col = "darkgreen")
hist(studentSuccess$Curricular.units.1st.sem..without.evaluations., col = "darkseagreen")
hist(studentSuccess$Curricular.units.2nd.sem..enrolled., col = "darkolivegreen")
#hist(Curricular.units.2nd.sem..approved.)
hist(studentSuccess$Curricular.units.2nd.sem..without.evaluations., col = "darkslategrey")
#hist(Inflation.rate)
# Getting indices of columns that do contain continuous variables so we can find the skewness of these variables
continuous_cols <- sapply(studentSuccess, is.numeric)
skew.values <- apply(studentSuccess[ ,continuous_cols], 2, skewness)
print(skew.values)
# will need transformation
# make categorical dummy vars
dummies <- dummyVars(~ ., data = success_predictors)
success_new_pred <- predict(dummies, success_predictors)
# only numeric values
success_pred_numeric <- success_predictors %>% select_if(is.numeric) # select only numeric predictors
corrplot(cor(success_pred_numeric))
# check for multicollinearity
corr_pred <- cor(success_new_pred)
corrplot(corr_pred)
corr_pred2 <- cor(continuous_cols)
# Loading in needed packages
library(caret)
library(moments)
library(corrplot)
library(AppliedPredictiveModeling)
library(ggplot2)
library(e1071)
library(glmnet)
library(MASS)
library(pamr)
library(pROC)
library(sparseLDA)
library(dplyr)
# Reading data into dataframe, data is seperated by semicolon so using that as delimeter
setwd("/Users/crorick/Documents/MS\ Applied\ Stats\ Fall\ 2023/MA5790/group_project/R_code")
studentSuccess <- read.csv('data.csv', sep = ';', header = TRUE)
# Getting a view of what the data looks like
str(studentSuccess)
studentSuccess$Target
# check distribution of response
par(mfrow = c(1,1))
hist(as.numeric(factor(studentSuccess$Target)), col = "steelblue",
main = "Distribution of Student Success",
xlab = "(1 = Dropout, 2 = Enrolled, 3 = Graduate)")
# Histogram shows very different frequencies for the various levels, a stratified sampling approach should be taken
#### pre-processing ###
# check for missing values
nas_bycol <- colSums(is.na(studentSuccess))
sum(is.na(studentSuccess))
# no missing values
# separate response from predictors
success_ouctome <- studentSuccess$Target # saves outcome separately
success_predictors <- studentSuccess
success_predictors$Target <- NULL # removes outcome from predictor set
str(success_predictors)
str(success_ouctome)
success_ouctome <- as.factor(success_ouctome)
## check skewness
# histograms of predictors
par(mfrow = c(3,4))
#hist(Previous.qualification, col = "lightblue")
hist(studentSuccess$Admission.grade, col = "darkblue")
hist(studentSuccess$Curricular.units.1st.sem..credited., col = "cornflowerblue")
hist(studentSuccess$Curricular.units.1st.sem..evaluations., col = "darkorange")
hist(studentSuccess$Curricular.units.1st.sem..grade., col = "darkred")
hist(studentSuccess$Curricular.units.2nd.sem..credited., col = "yellow")
# hist(Curricular.units.2nd.sem..evaluations.) # not skewed trying to lower margins
hist(studentSuccess$Curricular.units.2nd.sem..grade., col = "darkslateblue")
#hist(Unemployment.rate)
#hist(GDP)
#hist(Previous.qualification..grade.)
hist(studentSuccess$Age.at.enrollment, col = "darkmagenta")
hist(studentSuccess$Curricular.units.1st.sem..enrolled., col = "purple")
par(mfrow = c(2,2))
hist(studentSuccess$Curricular.units.1st.sem..approved., col = "darkgreen")
hist(studentSuccess$Curricular.units.1st.sem..without.evaluations., col = "darkseagreen")
hist(studentSuccess$Curricular.units.2nd.sem..enrolled., col = "darkolivegreen")
#hist(Curricular.units.2nd.sem..approved.)
hist(studentSuccess$Curricular.units.2nd.sem..without.evaluations., col = "darkslategrey")
#hist(Inflation.rate)
# Getting indices of columns that do contain continuous variables so we can find the skewness of these variables
continuous_cols <- sapply(studentSuccess, is.numeric)
skew.values <- apply(studentSuccess[ ,continuous_cols], 2, skewness)
print(skew.values)
str(studentSuccess)
# Loading in needed packages
library(caret)
library(moments)
library(corrplot)
library(AppliedPredictiveModeling)
library(ggplot2)
library(e1071)
library(glmnet)
library(MASS)
library(pamr)
library(pROC)
library(sparseLDA)
library(dplyr)
# Reading data into dataframe, data is seperated by semicolon so using that as delimeter
setwd("/Users/crorick/Documents/MS\ Applied\ Stats\ Fall\ 2023/MA5790/group_project/R_code")
studentSuccess <- read.csv('data.csv', sep = ';', header = TRUE)
# Getting a view of what the data looks like
str(studentSuccess)
studentSuccess$Target
# check distribution of response
par(mfrow = c(1,1))
hist(as.numeric(factor(studentSuccess$Target)), col = "steelblue",
main = "Distribution of Student Success",
xlab = "(1 = Dropout, 2 = Enrolled, 3 = Graduate)")
# Histogram shows very different frequencies for the various levels, a stratified sampling approach should be taken
#### pre-processing ###
# check for missing values
nas_bycol <- colSums(is.na(studentSuccess))
sum(is.na(studentSuccess))
# no missing values
# separate response from predictors
success_ouctome <- studentSuccess$Target # saves outcome separately
success_predictors <- studentSuccess
success_predictors$Target <- NULL # removes outcome from predictor set
str(success_predictors)
str(success_ouctome)
success_ouctome <- as.factor(success_ouctome)
## check skewness
# histograms of predictors
par(mfrow = c(3,4))
#hist(Previous.qualification, col = "lightblue")
hist(studentSuccess$Admission.grade, col = "darkblue")
hist(studentSuccess$Curricular.units.1st.sem..credited., col = "cornflowerblue")
hist(studentSuccess$Curricular.units.1st.sem..evaluations., col = "darkorange")
hist(studentSuccess$Curricular.units.1st.sem..grade., col = "darkred")
hist(studentSuccess$Curricular.units.2nd.sem..credited., col = "yellow")
# hist(Curricular.units.2nd.sem..evaluations.) # not skewed trying to lower margins
hist(studentSuccess$Curricular.units.2nd.sem..grade., col = "darkslateblue")
#hist(Unemployment.rate)
#hist(GDP)
#hist(Previous.qualification..grade.)
hist(studentSuccess$Age.at.enrollment, col = "darkmagenta")
hist(studentSuccess$Curricular.units.1st.sem..enrolled., col = "purple")
par(mfrow = c(2,2))
hist(studentSuccess$Curricular.units.1st.sem..approved., col = "darkgreen")
hist(studentSuccess$Curricular.units.1st.sem..without.evaluations., col = "darkseagreen")
hist(studentSuccess$Curricular.units.2nd.sem..enrolled., col = "darkolivegreen")
#hist(Curricular.units.2nd.sem..approved.)
hist(studentSuccess$Curricular.units.2nd.sem..without.evaluations., col = "darkslategrey")
#hist(Inflation.rate)
# Getting indices of columns that do contain continuous variables so we can find the skewness of these variables
continuous_cols <- sapply(studentSuccess, is.numeric)
skew.values <- apply(studentSuccess[ ,continuous_cols], 2, skewness)
print(skew.values)
# will need transformation
# make categorical dummy vars
dummies <- dummyVars(~ ., data = success_predictors)
success_new_pred <- predict(dummies, success_predictors)
# only numeric values
success_pred_numeric <- success_predictors %>% select_if(is.numeric) # select only numeric predictors
corrplot(cor(success_pred_numeric))
# check for multicollinearity
corr_pred <- cor(success_new_pred)
corrplot(corr_pred)
corr_pred2 <- cor(continuous_cols)
?trainControl()
